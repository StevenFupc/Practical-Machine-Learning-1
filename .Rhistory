gdp <- gdp[c(-1,-2,-4),]
#rename variable name
names(gdp)[names(gdp) == "X"] <- "CC" #CC means CountryCode
names(gdp)[names(gdp) == "Gross.domestic.product.2012"] <- "Ranking"
names(gdp)[names(gdp) == "X.2"] <- "Economy"
names(gdp)[names(gdp) == "X.3"] <- "UD"
#remove row 1
gdp <- gdp[-1,]
#subset gdp, new gdp include 190 countries and 4 variables
gdp <- gdp[1:190, c(1,2,4,5)]
#load Edu file
edu <- read.csv("./GCD/q3.3Edu.csv")
#Merge gdp and edu by merge function. all = FALSE indicate only find the match countrycode
m3 <- merge(gdp, edu, by.x = "CC", by.y = "CountryCode", all = FALSE)
View(gdp)
View(`m3`)
length(grep("June", m3$Special.Notes))
length(grep("end: June", m3$Special.Notes))
install.packages("quantmod")
library(quantmod)
amzn = getSymbols("AMZN",auto.assign=FALSE)
sampleTimes = index(amzn)
View(amzn)
length(grep("^2012", amzn$row.names))
amzn$date <- amzn$row.names
View(amzn)
amzn = getSymbols("AMZN",auto.assign=FALSE)
sampleTimes = index(amzn)
View(amzn)
str(amzn)
?write.csv
write.csv(amzn, file = "./GCD/amzn.csv")
q4.5 <- read.csv("./GCD/amzn.csv")
View(`q4.5`)
remove(q4.5)
q4.5 <- read.csv("./GCD/amzn1.csv", row.names = TRUE)
write.csv(amzn, file = "./GCD/amzn1.csv", row.names = TRUE)
q4.5 <- read.csv("./GCD/amzn1.csv")
View(`q4.5`)
remove(q4.5)
amznsubset <- amzn[1100:1929]
View(amznsubset)
index(amzn)
length(grep("^2012", index(amzn)))
x <- grep("^2012", index(amzn))
x
amzn$day <- weekdays(index(amzn))
View(amzn)
amzn$day <- weekdays(as.Date(index(amzn)))
View(amzn)
names(amzn)
length(grep("^2012", index(amzn)) & amzn&day == "Monday")
length(grep("^2012 & amzn&day == "Monday"", index(amzn)))
amzn$day <- as.Date(index(amzn), "%d%b%Y%A")
View(amzn)
amzn$day <- weekdays(as.Date(index(amzn), "%d%b%Y%A"))
View(amzn)
nrows(amzn$day == "Monday" & grep("^2012", index(amzn)))
?nrows
sum(amzn$day == "Monday" & grep("^2012", index(amzn)))
amznsubset <- amzn(grep("^2012", index(amzn)))
amznsubset <- amzn[grep("^2012", index(amzn))]
length(grep("Monday", amznsubset$day))
library(quantmod)
amzn = getSymbols("AMZN",auto.assign=FALSE)
sampleTimes = index(amzn)
length(grep("^2012", index(amzn)))
#Add new col day to amzn
amzn$day <- weekdays(as.Date(index(amzn)))
#subset amzn only with 2012 data
amzn2012 <- amzn[grep("^2012", index(amzn))]
#Calculate number of Monday
length(grep("Monday", amzn2012$day))
View(amzn2012)
amzn$day <- as.Date(index(amzn))
View(amzn)
amzn$day <- weekdays(as.Date(index(amzn)))
View(amzn)
#Q5
#Install package quantmod
#load package quantmod
library(quantmod)
amzn = getSymbols("AMZN",auto.assign=FALSE)
sampleTimes = index(amzn)
length(grep("^2012", index(amzn)))
#Add new col day to amzn
amzn$day <- weekdays(as.Date(index(amzn)))
#subset amzn only with 2012 data
amzn2012 <- amzn[grep("^2012", index(amzn))]
#Calculate number of Monday
length(grep("Monday", amzn2012$day))
library(quantmod)
amzn = getSymbols("AMZN",auto.assign=FALSE)
sampleTimes = index(amzn)
length(grep("^2012", index(amzn)))
#Add new col day to amzn
amzn$day <- as.Date(index(amzn))
amzn$day <- weekdays(as.Date(index(amzn)))
#subset amzn only with 2012 data
amzn2012 <- amzn[grep("^2012", index(amzn))]
#Calculate number of Monday
length(grep("Monday", amzn2012$day))
install.packages("openintro")
install.packages("OIdata")
x < -c(1:6)
x <- c(1:6)
sd(x)
x < c(0,1,3,3,3,5,6)
x <- c(0,1,3,3,3,5,6)
sd(x)
x <- c(100, 100, 100, 100, 100, 100, 101)
sd(x)
x <- c(
0, 25, 50, 100, 125, 150, 1000)
sd(x)
sd(c(0, 100, 200, 300, 400, 500, 600))
USP <- read.table("http://s3.amazonaws.com/assets.datacamp.com/course/dasi/present.txt")
head(USP)
View(USP)
dim(USP)
names(USP)
plot(USP$girls, USP$year)
abline(USP$girls, USP$year)
abline(USP$girls, USP$year, type = "l")
plot(USP$girls, USP$year)
plot(USP$girls, USP$year)
abline(USP$girls, USP$year, type = "l")
abline(USP$girls, USP$year, "l")
plot(USP$girls, USP$year, type = "l")
plot(USP$girls, USP$year, type = "l")
plot(USP$year, USP$girls, type = "l")
babies <- USP$boys + USP$girls
plot(USP$year, babies, type = "l")
plot(USP$year, USP$boys / USP$girls, type = "l")
max(USP$boys - USP$girls)
USP$boys - USP$girls
library("swirl")
install_from_swirl("Getting and Cleaning Data")
swirl()
mydf <- read.csv("path2csv.csv", stringsAsFactors = FALSE)
mydf <- read.csv(path2csv, stringsAsFactors = FALSE)
dim(mydf)
head(mydf)
library(dplyr)
packageVersion("dplyr")
cran <- tbl_df(mydf)
rm("mydf")
cran
?mainp
?manip
select(cran, ip_id, package, country)
5:20
select(cran, r_arch:country)
select(cran, country:r_arch)
cran
select(cran, -time)
cran
-5:20
-(5:20)
select(cran, -(X:size))
filter(cran, package == "swirl")
filter(cran, r_version == "3.1.1", country == "US")
?Comparison
filter(cran, r_version <= "3.0.2", country == "IN")
filter(cran, country == "US" | country == "IN")
filter(cran, size > 100500, r_os == "linux-gnu")
is.na(c(3,5,NA,10))
!is.na(c(3,5,NA,10))
filter(cran, !is.na(r_version))
cran
cran2 <- select(cran, size:ip_id)
arrange(cran2, ip_id)
arrange(cran2, desc(ip_id))
arrange(cran2, package, ip_id)
cran2
arrange(cran2, country, des(r_version), ip_id)
arrange(cran2, country, desc(r_version), ip_id)
cran3 <- select(cran, ip_id, package, size)
cran3
mutate(cran3, size_mb = size / 2^20)
mutate(cran3, size_gb = size_mb / 2^10)
mutate(cran3, size_gb = size_mb / 2^10)
mutate(cran3, size_gb = size_mb/2^10)
mutate(cran3, size_gb = (size / 2^20)/ 2^10)
mutate(cran3, size_mb = size/2^20, size_gb = size_mb / 2^10)
mutate(cran, correct_size = size - 1000)
mutate(cran3, correct_size = size + 1000)
summarize(cran, avg_bytes = mean(size))
swirl()
library(swirl)
swirl()
install_from_swirl("Getting and Cleaning Data")
library(swirl)
swilr()
swirl()
swirl()
swirl()
swirl()
Sys.getlocal("LC_TIME")
Sys.getlocale("LC_TIME")
library(lubridate)
help(package = lubridate)
today()
this_day <- today()
this_day
year(this_day)
wday(this_day)
wday(this_day, label = TRUE)
this_moment <- now()
this_moment
minute(this_moment)
ymd("1989-05-17")
my_date <- ymd("1989-05-17")
my_date
class(my_date)
ymd("1989 May 17")
ymd("March 12, 1975")
ymd("March 12 1975")
info()
mdy("March 12, 1975")
dmy(25081985)
ymd("192012")
ymd("1920-1-2")
dt1
ymd_hms(dt1)
hms("03:22:14")
dt2
ymd("2014-05-14", "2014-09-22", "2014-07-11")
ymd(dt2)
update(this_moment, hours = 8, minutes = 34, seconde = 55)
update(this_moment)
update(this_moment, hours = 8, minutes = 59, seconds = 55)
update(this_moment, hours = 8, minutes = 34, seconde = 55).
update(this_moment, hours = 8, minutes = 34, secondes = 55)
update(this_moment, hours = 8, minutes = 34, seconds = 55)
this_moment
update(this_moment)
update(this_moment, hours = 10, minutes = 16, seconds = 0)
this_moment <- update(this_moment, hours = 10, minutes = 16, seconds = 0)
this_moment
nyc <- now("America/New_York")
nyc
depart <- nyc + days(2)
depart
depart <- update(depart, hours = 17, minutes = 34)
depart
arrive <- depart + hours(15) + minutes(50)
?with_tz
arrive <- with_tz(arrive, tzone = "Asia/Hong_Kong")
arrive
last_time <- with_tz(mdy("June 17, 2008"), tzone = "Singapore")
last_time <- mdy("June 17, 2008", tzone = "Singapore")
last_time <- mdy("June 17, 2008", tz = "Singapore")
last_time
new_interval(arrive, last_time)
?new_interval
how_long <- new_interval(last_time, arrive, tzone = attr(last_time, "tzone"))
how_long <- new_interval(last_time, arrive)
as.period(how_long)
stopwatch()
library(swirl)
swirl()
sessioninfo()
sessionInfo()
update.packages()
library(swirl)
swirl()
install_from_swirl("Getting and Cleaning Data")
swirl()
sessionInfo()
swirl()
swirl()
getwd()
Q1
#change file name to q1
#load q1
q1 <- read.csv("./GCD/q1.csv")
#summary VAL, 24 meaning value >= 1,000,000
table(q1$VAL)
Q2
#FES = Family type and employment status
#Family type should be one variable (personal thought)
#employment status should be another variable (personal thought)
Q3
#change file name to q3
#set row and col index
#load q3
colIndex <- 7:15
rowIndex <- 18:23
dat <- read.xlsx("./GCD/q3.xlsx", sheetIndex = 1, colIndex = colIndex, rowIndex = rowIndex)
sum(dat$Zip*dat$Ext,na.rm=T)
Q4
#method 1
fileUrl <- "http://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Frestaurants.xml"
doc <- xmlInternalTreeParse(fileUrl, useInternal = TRUE)
rootNode <- xmlRoot(doc)
table(xpathSApply(rootNode, "//zipcode", xmlValue))
#method 2, silly method
#open the xml page in Chrome, cmd+f, search 21231
Q5
#change file name to q5.csv
#load file by fread() function
DT <- fread("./GCD/q5.csv")
names(DT)
#calculate the average value of pwgtp15 by sex
DT[, mean(pwgtp15), by = SEX]
getwd()
rm(andy)
library(swirl)
swril)_
swirl(0
swirl()
x <- 4 /18
x
y <- 14 / 18
y
z <- y - x
z
rc <- 38 / 51
rc
rc <- 38 / 89
rc
rs <- 51 / 89
rs
rg <- rs - rc
rg
35/290
146/341
35/250
146/250
rm(andy)
library(hflights)
# explore data
data(hflights)
flights <- tbl_df(hflights)
flights %>%
group_by(UniqueCarrier)
library(hflights)
library(dplyr)
# explore data
data(hflights)
flights <- tbl_df(hflights)
flights %>%
group_by(UniqueCarrier)
flights <- flights %>%
group_by(UniqueCarrier)%>%
summarise_each(funs(mean), Cancelled, Diverted)
flights
re(andy)
rm(andy)
download.file(url = "http://bit.ly/dasi_project_template", destfile = "dasi_project_template.Rmd")
rm(andy)
install.packages("gridExtra")
rm(andy)
fileURL <- "http://data.dft.gov.uk.s3.amazonaws.com/nptdr/October-2011.zip"
download.file(fileURL)
download.file(fileURL, transport)
1 ^ 0
10 ^ 0
0.5 ^ 10
11 * (0.5 ^ 10)
1.645*50+1020
ppois(40, lambda = 9 * 5)
sqrt((0.5*0.5)/100)
install.packages("caret", dependencies = c("Depends", "Suggests"))
install.packages("kernlab")
library(AppliedPredictiveModeling)
install.packages("AppliedPredictiveModeling")
library(AppliedPredictiveModeling)
data(concrete)
library(caret)
library(lattice)
install.packages("BradleyTerry2")
library(caret)
install.packages("nloptr")
library(caret)
set.seed(975)
setwd("~/Practical-Machine-Learning")
traindata <- read.csv("pml-training.csv")
testdata <- read.csv("pml-testing.csv")
View(traindata)
traindata <- read.csv("pml-training.csv", na.strings = c("NA", "", "#DIV/0!"))
testdata <- read.csv("pml-testing.csv", na.strings = c("NA", "", "#DIV/0!"))
View(traindata)
View(traindata)
traindatacopy <- traindata
traindatacopy <- traindatacopy[complete.cases(traindatacopy),]
rm(traindatacopy)
traindatacopy <- traindata
traindatacopy <- traindatacopy[, -1:7]
traindatacopy <- traindatacopy[, -c(1:7)]
View(traindatacopy)
table(traindatacopy$classes)
table(traindatacopy$class)
names(traindatacopy)
str(traindatacopy)
dim(traindatacopy)[2]
dim(traindata)[2]
dim(traindata)[7]
dim(traindata)[8]
dim(traindata)
dim(traindata)[1]
table(round(apply(traindatacopy, 2, function(x) sum(is.na(x)))/nrow(traindatacopy), 2))
columns.naCount <- apply(data, 2, function(x) sum(is.na(x)))
columns.naCount <- apply(traindatacopy, 2, function(x) sum(is.na(x)))
columns.naCount
isna <- apply(traindatacopy, 2, sum(is.na()))
isna <- apply(traindatacopy, 2, sum(is.na(x)))
isna <- apply(traindatacopy, 2, sum(is.na(traindatacopy[, c(1:153)])))
traindata1 <- read.csv("pml-training.csv", na.strings = c("NA",""), strip.white=TRUE)
View(traindata1)
View(traindatacopy)
rm(traindata1)
sapply(traindatacopy, 2, function(x) sum(is.na(x)))
apply(traindatacopy, 2, function(x) sum(is.na(x)))
unlist(apply(traindatacopy, 2, function(x) sum(is.na(x))))
unlist(apply(traindatacopy, 2, function(x) sum(is.na(x))))/nrow(traindatacopy)
round(unlist(apply(traindatacopy, 2, function(x) sum(is.na(x))))/nrow(traindatacopy), 2)
table(round(unlist(apply(traindatacopy, 2, function(x) sum(is.na(x))))/nrow(traindatacopy), 2)
)
apply(traindatacopy, 2, function(x) sum(is.na(x)))
table(apply(traindatacopy, 2, function(x) sum(is.na(x))))
summarise(apply(traindatacopy, 2, function(x) sum(is.na(x))))
summarize(apply(traindatacopy, 2, function(x) sum(is.na(x))))
countna <- table(apply(traindatacopy, 2, function(x) sum(is.na(x))))
traindatacopy <- traindatacopy[, countna == 0]
View(traindatacopy)
traindatacopy <- traindata
rm(columns.naCount)
rm(countna)
countna <- apply(traindatacopy, 2, function(x) sum(is.na(x)))
length(names(countna))
names(countna) == 0
names(countna)
countna
class(countna)
table(countna)
class(names(countna))
rm(countna)
countna <- as.matrix(apply(traindatacopy, 2, function(x) sum(is.na(x))))
countna
str(countna)
dim(countna)
rm(countna)
countna <- sapply(traindatacopy, 2, function(x) sum(is.na(x)))
countna <- lapply(traindatacopy, 2, function(x) sum(is.na(x)))
countna <- apply(traindatacopy, 2, function(x) sum(is.na(x)))
names(countna)[countna]
names(countna)[countna]==0
countna
names(countna)[countna==0]
names(countna)$countna == 0
countna <- apply(traindata, 2, function(x) sum(is.na(x)))
nona <- names(countna)[countna==0]
nona
traindata <- traindata[, nona]
countnacopy <- apply(traindatacopy, 2, function(x) sum(is.na(x)))
nonacopy <- names(countnacopy)[countnacopy == 0]
traindatacopy <- traindatacopy[, nonacopy]
nearZeroVar(traindatacopy, saveMetrics = T)
library(caret)
nearZeroVar(traindatacopy, saveMetrics = TRUE)
nearZeroVar(iris[, -5], saveMetrics = TRUE)
traindata <- traindata[, -c(1:7)]
library(tree)
library(randomForest)
library(corrplot)
View(testdata)
testdata <- testdata[,nona]
library(caret)
library(tree)
library(randomForest)
library(corrplot)
traindata <- read.csv("pml-training.csv", na.strings = c("NA", "", "#DIV/0!"))
testdata <- read.csv("pml-testing.csv", na.strings = c("NA", "", "#DIV/0!"))
rm(countna)
rm(countnacopy)
rm(nona)
rm(nonacopy)
trainna <- apply(traindata, 2, function(x) sum(is.na(x)))
trainnona <- names(trainna)[trainna==0]
traindata <- traindata[, trainnona]
traindata <- traindata[, -c(1:7)]
testna <- apply(testdata, 2, function(x) sum(is.na(x)))
testnona <- names(testna)[testna==0]
testdata <- testdata[, testnona]
testdata <- testdata[, -c(1:7)]
inTrain <- createDataPartition(y=traindata$classe, p=.60, list=FALSE)
trainingdata <- traindata[inTrain,]
testingdata <- traindata[-inTrain,]
rm(traindatacopy)
training.cv <- traindata[-inTrain,]
rm(testingdata)
set.seed(6897)
folds <- createFolds(y=traindata$classe,k=10, list=TRUE,returnTrain=TRUE)
sapply(folds,length)
fit <- randomForest(classe ~ ., data = trainingdata, method = "class")
predictor <- predict(fit, training.cv)
confusionMatrix(predictor, trining.cv$classe)
confusionMatrix(predictor, training.cv$classe)
confusionMatrix(predictor, training.cv$classe)$table
sum(predictor == training.cv$classe) / length(predictor)
destfold = "~/Practical-Machine-Learning/submission"
results <- predict(fit, newdata = testdata)
pml_write_files = function(x){
n = length(x)
for(i in 1:n){
filename = paste0(destfold, "problem_id_", i, ".txt")
write.table(x[i], file = filename, quote = FALSE, row.names = FALSE,col.names = FALSE)
}
}
pml_write_files(results)
